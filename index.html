<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>The Shea Blog</title>
  <style>
    html, body { margin: 0; padding: 0; background: #f4f1ea; color: #222; font-family: Georgia, 'Times New Roman', Times, serif; line-height: 1.7; }
    a { color: #06c; text-decoration: underline; }
    a:visited { color: #551a8b; }
    .wrap { max-width: 760px; margin: 0 auto; padding: 24px 16px; }
    header { border: 1px solid #c9c3b8; background: #fffefa; padding: 24px; margin-bottom: 20px; }
    .site-title { font-size: 32px; margin: 0 0 6px; }
    nav { padding: 12px 0; border-top: 1px solid #e0dbcf; }
    nav button.tab { border: 0; background: transparent; padding: 10px 12px; margin-right: 8px; font: inherit; color: #06c; text-decoration: underline; cursor: pointer; }
    nav [aria-selected='true'] { color: #111; text-decoration: none; }
    main { background: #fff; border: 1px solid #d9d2c4; padding: 32px; margin-top: 20px; }
    .card { background:#fff; border: 1px solid #e6dfd2; padding: 32px; margin: 22px 0; }
    article h2 { font-size: 22px; margin-top: 0; }
    p { margin: 16px 0; }
    footer { border: 1px solid #c9c3b8; padding: 16px 20px; color: #555; background: #fffefa; margin-top: 20px; }
    [role='tabpanel'] { opacity: 0; visibility: hidden; height: 0; overflow: hidden; transition: opacity .3s ease, transform .3s ease; transform: translateY(6px); }
    [role='tabpanel'].active { opacity: 1; visibility: visible; height: auto; overflow: visible; transform: translateY(0); }
  </style>
</head>
<body>
  <div class="wrap">
    <header>
      <h1 class="site-title">The Shea Blog</h1>
      <div class="small">An ENC 1102 research blog on technology and human connection</div>
      <nav role="tablist" aria-label="Site tabs">
        <button class="tab" role="tab" aria-selected="true" aria-controls="draft2" id="tab-draft2">Draft 2</button>
        <button class="tab" role="tab" aria-selected="false" aria-controls="draft1" id="tab-draft1">Draft 1</button>
        <button class="tab" role="tab" aria-selected="false" aria-controls="works" id="tab-works">Works Cited</button>
        <button class="tab" role="tab" aria-selected="false" aria-controls="reflection" id="tab-reflection">Reflection</button>
      </nav>
    </header>

    <main>
      <!-- Draft 2 tab -->
      <section id="draft2" role="tabpanel" class="active" aria-labelledby="tab-draft2">
        <div class="card">
          <article>
            <h2>How reliance on AI shapes human connection (Draft 2)</h2>
            <div class="meta">By Brennan Shea. November 12, 2025.</div>
            <p>I use artificial intelligence every day. It drafts emails, plans tasks, and finds patterns in data. These tools feel normal now. The question is how this reliance changes the way people relate. AI can lift productivity and widen access, but dependence can weaken human connection. The aim is measured use. Within the next year, schools, workplaces, and public agencies should label AI-assisted work, limit undisclosed automation in judgments, and require human review for high-stakes decisions.</p>
            <p>The appeal of AI is practical. Many people use it to finish repetitive work quickly and with fewer errors. It drafts notes, summarizes sources, and catches grammar problems, which saves time and frustration. Lawson reports similar gains for everyday users, improved communication and creative support (Lawson). From my experience, using AI to outline schedules and build rough drafts frees attention for harder thinking and better conversations. Efficiency is real. It is one of the technology’s strongest benefits.</p>
            <p>The same advantages can serve the public good. In Southeast Asia, machine learning has improved weather prediction, forest monitoring, and waste management. Board documents these cases and notes the promise despite infrastructure limits (Board). Some readers will argue that successes like these prove the social value of rapid AI rollout. That gain matters. Yet scale without clear disclosure and oversight can hide risks, which undermines trust and makes later correction harder.</p>
            <p>Still, the benefits come with serious ethical issues. The European Parliamentary Research Service warns that unregulated AI increases surveillance, bias, and privacy loss (European Parliamentary Research Service). When organizations automate decisions, small biases can widen into inequality. People who feel watched speak less and create less. If that happens, a tool meant to empower ends up eroding trust. Without guardrails, the systems that should connect us can divide us.</p>
            <p>There is also a risk to critical thinking. Students who use chatbots for quick answers lose practice in analysis. Professionals who rely on assistants for feedback lose practice in empathy. Russell and Norvig note that intelligent reasoning depends on context, something machines still struggle to grasp (Russell and Norvig). If people outsource too much judgment, they slowly lose skills that make human decisions valuable.</p>
            <p>Another problem involves information quality. Bender and colleagues describe the stochastic parrots issue, fluent but unreliable text (Bender et al.). The systems can mimic understanding while repeating bias and error. The result is polished writing without accuracy. When readers cannot tell truth from imitation, civic understanding suffers. Confusion harms classrooms and communities.</p>
            <p>Social scientists warn about emotional distance. Turkle shows how people swap digital talk for real conversation (Turkle). AI can accelerate that swap by managing emotional exchanges for us. A chatbot can write apologies, compliments, and condolences. The words look fine, but they lack the vulnerability that builds trust. Over time, outsourcing feelings can dull empathy and strain relationships.</p>
            <p>Technology is not the only cause. Vincent argues that human priorities shape outcomes as much as code (Vincent). A meeting becomes shallow when leaders value speed over care. A class discussion loses meaning when students choose convenience over curiosity. Design and use are inseparable. Developers and users share responsibility for making tools serve human well-being.</p>
            <p>Balancing these views calls for intention. Use AI to assist, not replace, human insight. Boundaries protect integrity. Let AI summarize reports or collect data. Keep moral and creative choices with people. Label AI-assisted content so readers understand how it was made. Transparency supports accountability and better engagement.</p>
            <p>Privacy needs protection too. The OECD stresses transparency, data minimization, and human oversight as core safeguards (OECD). Governments and companies should restrict unnecessary data collection and enforce fairness audits. Smart regulation does not block progress. It makes innovation safer and more sustainable. These standards keep AI anchored to human dignity.</p>
            <p>Practice also matters. Teams should reserve time for live problem solving, debate, and mentorship. Students should draft without AI, then use it for revision with a short process note that explains how. These habits protect creativity and empathy. Workplaces and classrooms thrive when dialogue, not automation, drives the hardest parts of the work.</p>
            <p>Public institutions can still use AI for safety, infrastructure, and access. Human oversight must remain central. Share reports on how systems operate and invite citizen feedback. Treat the public as collaborators, not subjects. Openness builds trust and keeps innovation aligned with shared values.</p>
            <p>Some worry that guardrails will slow innovation and put communities at a disadvantage compared to less cautious competitors. That fear is understandable, especially in fields where speed becomes a proxy for progress. But trust is also an economic asset. If people understand how systems work, what data they use, and how to challenge outcomes, adoption becomes easier, not harder. Responsible process reduces backlash and rework. In the long run, it saves time.</p>
            <p>Consider education in more detail. In classes that permit AI, clear norms reduce confusion and stress. Students can use assistants to brainstorm or check grammar, but they still write the core analysis themselves. A short statement at the end of each submission can explain what the tool did, what the student did, and where judgment came from. That note turns a hidden shortcut into a visible choice. It also gives teachers a better view of the process, which is useful feedback for instruction.</p>
            <p>The workplace has parallel needs. Teams that rely on AI for planning or drafting should treat the system like a junior collaborator: helpful, fast, and never the final voice. Leaders can schedule regular “human-only” sessions for brainstorming or retrospective conversations. In those moments, people practice listening and argument without the net of automatic suggestions. The contrast is healthy. It reminds teams that their value lies in insight and care, not just throughput.</p>
            <p>High-stakes decisions deserve special protection. If a model screens loans, grades essays, flags medical images, or prioritizes social services, it should not operate in the dark. People who are affected need to know how to ask questions, request review, and appeal outcomes to a person with authority to change them. That path should be simple and fast. Otherwise, the appearance of efficiency wins while fairness and dignity lose.</p>
            <p>Culture shapes all of this. Tools do not decide whether a community values speed over understanding or numbers over nuance. People do. When leaders choose metrics that treat conversation as waste, relationships thin out. When institutions reward reflection, people make time for it. The same technology can support either outcome. The difference is not the code, but the priorities around it.</p>
            <p>There are also concrete design choices that keep AI relational instead of isolating. One choice is friction. If an assistant writes a response for you, the interface can require a short edit before sending. That pause invites voice and accountability. Another choice is visibility. Systems can show the sources behind a claim, not just a polished paragraph. A third choice is reciprocity. Instead of doing tasks invisibly, tools can teach while helping, explaining why a suggestion fits and what tradeoffs it carries. Each of these choices keeps the person in the loop.</p>
            <p>Critics sometimes argue that people already outsource judgment to institutions and experts, so worrying about AI is just nostalgia. But the scale and speed of computation make this wave different. A single model can shape millions of interactions in a day, often with a tone that sounds authoritative even when it is uncertain. That reach demands higher standards for evidence, consent, and correction than most past tools required.</p>
            <p>It is also worth naming the equity angle. If only well-resourced schools and workplaces can afford thoughtful guardrails, then the benefits of AI will cluster with privilege while risks concentrate elsewhere. That pattern already appears in other technologies, from broadband to digital identification. Shared standards for disclosure and review reduce the chance of a two-tier system in which some people get human attention and others get automated indifference.</p>
            <p>None of this implies that AI should stay in a lab. The promise is real: faster access to expertise, more consistent service delivery, new ways to learn and create. The challenge is to pursue those gains without draining the human contact that gives work and learning their meaning. In practice, that means writing policies that are short, clear, and lived—not just filed. It means treating labels and process notes as habits, not hurdles. And it means honoring slowness at the moments where speed would break trust.</p>
            <p>The question is not whether AI helps or harms. The question is how we choose to use it. AI can expand knowledge, simplify tasks, and improve services. It can also dull empathy, spread bias, and narrow curiosity if left unchecked. The future depends on our willingness to slow down, label help, and stay accountable. Pair efficiency with ethics, and curiosity with care, so AI supports, not replaces, the human connection that defines us.</p>
            <p>My own habits have changed during this project. I still use AI for outlines and checks, but I try to pause before accepting the first draft. I ask whether the sentence sounds like me, whether the claim has a source I can read, and whether the suggestion would land well with the person on the other side. That pause takes a minute. It often saves an apology later. It feels like the difference between sending words and sending attention.</p>
          </article>
        </div>
      </section>

      <!-- Draft 1 tab -->
      <section id="draft1" role="tabpanel" aria-labelledby="tab-draft1">
        <div class="card">
          <article>
            <h2>How reliance on AI shapes human connection (Draft 1)</h2>
            <div class="meta">By Brennan Shea. November 3, 2025.</div>
            <p>I use artificial intelligence every day. It helps me draft emails, plan tasks, and find patterns in data. These tools feel natural now. Yet as their use expands, I often wonder how this growing reliance changes the way people relate to one another. Artificial intelligence can lift productivity and widen access, but unchecked dependence may weaken real human connection. The goal is not to reject AI but to manage its place responsibly in modern life.</p>
            <p>To understand the appeal of AI, it helps to look at its practical benefits. Many users rely on AI to complete repetitive tasks quickly and accurately. It drafts notes, summarizes information, and corrects grammar errors, saving people both time and frustration. Lawson highlights these effects, noting how AI enhances communication and creativity for everyday users (Lawson). From my experience, using AI to organize schedules and generate drafts frees mental space for deeper thought and more meaningful conversation. This efficiency represents one of the technology’s greatest strengths.</p>
            <p>These personal advantages also extend to public good. In Southeast Asia, machine learning has improved weather prediction, forest monitoring, and waste management. Board documents these examples, showing that while infrastructure limits remain, the results prove promising (Board). Projects like these demonstrate how AI can serve as a global tool for safety, sustainability, and innovation. When systems prevent harm or protect natural resources, they validate their role as an ally to human progress.</p>
            <p>However, these benefits are paired with serious ethical concerns. The European Parliamentary Research Service warns that unregulated AI use increases surveillance, bias, and privacy loss. As organizations automate decision-making, small biases can scale into widespread inequality. People who feel monitored may avoid open dialogue or creativity (European Parliamentary Research Service). In this way, technology that should empower users can instead suppress trust. Without oversight, the systems meant to connect us could divide us further.</p>
            <p>Beyond issues of privacy, dependence on AI can erode critical thinking. Students who turn to chatbots for quick answers lose practice in analysis. Professionals who use digital assistants to write feedback risk losing empathy. Russell and Norvig explain that intelligent reasoning depends heavily on context, something machines still struggle to grasp (Russell and Norvig). When people outsource judgment to AI, they slowly lose the very skills that make human decision-making valuable.</p>
            <p>Some experts argue this erosion extends to information itself. Bender and colleagues describe the “stochastic parrots” problem, where language models generate fluent but unreliable text (Bender et al.). While these systems mimic understanding, they often reproduce existing biases or errors. The result is polished writing without accuracy. When audiences cannot tell truth from imitation, civic understanding weakens. This confusion threatens democracy as much as individual learning.</p>
            <p>Social scientists also warn that AI can widen emotional distance. Turkle observes that people increasingly substitute digital conversation for genuine connection (Turkle). AI could accelerate this by managing emotional exchanges on our behalf. For example, users can ask a chatbot to write apologies, compliments, or condolences. The words may be polite, but they lack the personal vulnerability that builds trust. Over time, emotional outsourcing could dull empathy and make real relationships harder to maintain.</p>
            <p>Still, some believe that technology itself is not to blame. Vincent argues that human priorities shape AI outcomes, not the algorithms alone (Vincent). A meeting becomes shallow only when leaders value speed over care. A class discussion loses meaning only when students choose convenience over curiosity. This argument reminds us that design and use are inseparable. Developers and users share equal responsibility for ensuring that tools serve human well-being.</p>
            <p>Protecting privacy is equally crucial. The OECD emphasizes transparency and human oversight as essential safeguards (OECD). Governments and corporations must restrict unnecessary data collection and enforce fairness audits for algorithms. Responsible regulation does not limit progress; it ensures that innovation remains ethical and sustainable. These standards keep AI grounded in respect for human dignity.</p>
            <p>Just as systems need oversight, people need practice. Teams should reserve time for live problem-solving, debate, and mentorship. Students should write first drafts without AI before using it for revision. These steps preserve creativity and empathy. In both workplaces and classrooms, collaboration grows stronger when people rely on dialogue, not automation, to solve problems.</p>
            <p>Education shows how balance can succeed. Teachers can integrate AI as a brainstorming aid while reinforcing original thought. Asking students to explain how AI shaped their work builds accountability and reflection. These habits teach digital ethics and reinforce the value of authenticity in writing and discussion. When guided carefully, AI becomes a tool for growth, not avoidance.</p>
            <p>Public institutions face similar challenges. They can apply AI to improve public safety, manage infrastructure, and expand access to information. Yet human oversight must remain central. Publishing clear reports about how AI systems operate and inviting citizen feedback promote transparency. Treating the public as collaborators rather than subjects builds civic trust and ensures that innovation aligns with shared values.</p>
            <p>In the end, the question is not whether AI helps or harms but how humans choose to use it. Artificial intelligence magnifies both our strengths and our weaknesses. It can expand knowledge, streamline systems, and improve lives. It can also reduce empathy, spread bias, and dull creativity if left unchecked. The future depends on our willingness to slow down, reflect, and stay accountable. By pairing efficiency with ethics, and curiosity with care, we can ensure that AI supports—not replaces—the human connection that defines us.</p>
          </article>
        </div>
      </section>

      <section id="works" role="tabpanel" aria-labelledby="tab-works">
        <div class="card">
          <h2>Works Cited</h2>
          <ol class="works">
            <li>Board, Jack. "AI in Southeast Asia: Machine Learning Offers New Solutions to Age-old Environmental Problems." Channel News Asia, 2024.</li>
            <li>European Parliamentary Research Service. <em>The Ethics of Artificial Intelligence: Issues and Initiatives</em>. European Parliament, 2020.</li>
            <li>Lawson, BriA’nna. "Enhancing Everyday Life: How AI is Revolutionizing Your Daily Experience." CEAMLS News, Morgan State University, 21 Nov. 2023.</li>
            <li>Russell, Stuart, and Peter Norvig. <em>Artificial Intelligence: A Modern Approach</em>. 4th ed., Pearson, 2021.</li>
            <li>Vincent, James. "The Problem with AI Isn’t Technology, it’s Humans." <em>The Verge</em>, 2023.</li>
            <li>Bender, Emily M., et al. "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big." <em>Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency</em>, 2021.</li>
            <li>OECD. <em>OECD Principles on Artificial Intelligence</em>. Organisation for Economic Co-operation and Development, 2019.</li>
            <li>Turkle, Sherry. <em>Alone Together: Why We Expect More from Technology and Less from Each Other</em>. Basic Books, 2011.</li>
          </ol>
        </div>
      </section>

      <section id="reflection" role="tabpanel" aria-labelledby="tab-reflection">
        <div class="card">
          <h2>Reflection</h2>
          <p>Writing this project taught me how to connect evidence, purpose, and audience into one sustained argument. At first, I saw research writing as formal and distant, but ENC 1102 helped me see it as a process of choice and clarity. I had to decide what claim mattered, why readers should care, and how sources could guide reasoning without taking control of it. My goal was to balance analysis and simplicity so that the essay could read like a conversation about real consequences. Revision played the biggest role in growth. Early drafts felt mechanical. Later ones focused more on cause and effect, tone, and smooth transitions. I also learned to cut vague language and choose verbs that show intent. Citing sources correctly built credibility and rhythm. More than anything, I learned to treat writing as an act of thinking out loud, where each paragraph tests an idea against evidence. This reflection represents how ENC 1102 changed my understanding of writing from a grade-driven task to a form of inquiry and expression. It is about learning to write with both discipline and curiosity, which are the same skills that make responsible research possible.</p>
        </div>
      </section>
    </main>

    <footer>
      <div>© 2025 Brennan Shea.</div>
    </footer>
  </div>

  <script>
    const tabs = document.querySelectorAll('[role="tab"]');
    const panels = document.querySelectorAll('[role="tabpanel"]');
    function activate(id) {
      panels.forEach(p => {
        const isTarget = p.id === id;
        p.classList.toggle('active', isTarget);
        p.setAttribute('aria-hidden', String(!isTarget));
      });
      tabs.forEach(t => t.setAttribute('aria-selected', String(t.getAttribute('aria-controls') === id)));
      history.replaceState(null, '', '#' + id);
    }
    tabs.forEach(t => t.addEventListener('click', () => activate(t.getAttribute('aria-controls'))));
    const hash = location.hash.replace('#','');
    if (hash && document.getElementById(hash)) activate(hash);
  </script>
</body>
</html>
