<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>The Shea Blog</title>
  <style>
    html, body { margin: 0; padding: 0; background: #f4f1ea; color: #222; font-family: Georgia, 'Times New Roman', Times, serif; line-height: 1.7; }
    a { color: #06c; text-decoration: underline; }
    a:visited { color: #551a8b; }
    .wrap { max-width: 760px; margin: 0 auto; padding: 24px 16px; }
    header { border: 1px solid #c9c3b8; background: #fffefa; padding: 24px; margin-bottom: 20px; }
    .site-title { font-size: 32px; margin: 0 0 6px; }
    nav { padding: 12px 0; border-top: 1px solid #e0dbcf; }
    nav button.tab { border: 0; background: transparent; padding: 10px 12px; margin-right: 8px; font: inherit; color: #06c; text-decoration: underline; cursor: pointer; }
    nav [aria-selected='true'] { color: #111; text-decoration: none; }
    main { background: #fff; border: 1px solid #d9d2c4; padding: 32px; margin-top: 20px; }
    .card { background:#fff; border: 1px solid #e6dfd2; padding: 32px; margin: 22px 0; }
    article h2 { font-size: 22px; margin-top: 0; }
    p { margin: 16px 0; }
    footer { border: 1px solid #c9c3b8; padding: 16px 20px; color: #555; background: #fffefa; margin-top: 20px; }
    [role='tabpanel'] { opacity: 0; visibility: hidden; height: 0; overflow: hidden; transition: opacity .3s ease, transform .3s ease; transform: translateY(6px); }
    [role='tabpanel'].active { opacity: 1; visibility: visible; height: auto; overflow: visible; transform: translateY(0); }
  </style>
</head>
<body>
  <div class="wrap">
    <header>
      <h1 class="site-title">The Shea Blog</h1>
      <div class="small">An ENC 1102 research blog on technology and human connection</div>
      <nav role="tablist" aria-label="Site tabs">
        <button class="tab" role="tab" aria-selected="true" aria-controls="draft2" id="tab-draft2">Argument</button>
        <button class="tab" role="tab" aria-selected="false" aria-controls="draft1" id="tab-draft1">First Draft of Argument</button>
        <button class="tab" role="tab" aria-selected="false" aria-controls="works" id="tab-works">Works Cited</button>
        <button class="tab" role="tab" aria-selected="false" aria-controls="reflection" id="tab-reflection">Reflection</button>
      </nav>
    </header>

    <main>
      <!-- Draft 2 tab -->
      <section id="draft2" role="tabpanel" class="active" aria-labelledby="tab-draft2">
        <div class="card">
          <article>
            <h2>How reliance on AI shapes human connection</h2>
            <div class="meta">By Brennan Shea. November 17, 2025.</div>
            <p>I use artificial intelligence every day, and its presence shapes much of how I work. It drafts emails, plans tasks, and finds patterns that once took hours. As these tools blend into routine, they raise a deeper question about how reliance on AI changes the way people relate to one another. Conversations move faster, yet they sometimes feel thinner. Decisions come more quickly, yet people are not always sure who made them. This shift brings gains in productivity and access, but it also creates risks for trust, attention, and empathy. Because of this tension, schools, workplaces, and public agencies should label AI assisted work, limit hidden automation, and require human review for high stakes decisions.</p>
            <p>To understand this shift more clearly, it helps to begin with AI’s appeal. People use it to complete repetitive work with fewer errors, which creates an immediate sense of relief. BriA’nna Lawson, writing for Morgan State University’s CEAMLS News, explains how AI improves communication, supports health tasks, and encourages creativity (Lawson). These claims match my own experience. When AI handles early drafts and planning, I can focus on harder problems and deeper conversations. The tool becomes a steady partner that never tires of routine. As a result, adoption spreads quickly. Once people see that a chatbot can organize a schedule or revise a dense email, they find it difficult to return to older habits.</p>
            <p>These personal benefits extend into public life as well. Jack Board from Channel News Asia reports that machine learning improves weather prediction, forest monitoring, and waste management in Southeast Asia (Board). These improvements strengthen environmental resilience and public services. Better storm warnings give families more time to prepare. Smarter waste routes reduce fuel use and pollution. However, as these systems scale, concerns grow alongside them. When institutions rely on AI without disclosure or oversight, trust weakens. Citizens begin to wonder which choices reflect human judgment and which ones originate from opaque models they never see.</p>
            <p>These concerns become sharper when people do not realize AI is involved at all. If a public agency sends form letters generated by a model, the language may sound polite but detached. The recipient may feel unheard. Hidden automation can make government communication feel mechanical rather than responsive. Clear labels that acknowledge AI involvement would not solve every issue, but they would offer citizens a more honest understanding of how decisions are made.</p>
            <p>As reliance increases, ethical risks follow. The European Parliamentary Research Service warns that unregulated systems increase surveillance, bias, and privacy loss (European Parliamentary Research Service). These problems often grow quietly. Automated decisions can amplify small inequities. A loan model that misjudges the reliability of one neighborhood can deny thousands of families access to housing. A school that depends on automated risk scores may discipline certain students more harshly, even when teachers have no discriminatory intent. When people feel monitored, they speak less and create less. A tool designed to expand opportunity can restrict it when left unchecked.</p>
            <p>Beyond ethics, AI affects judgment and skill. Students who rely on chatbots for quick answers lose practice in analysis. They may complete assignments faster, but they skip the hard work of wrestling with uncertainty. Professionals who depend on automated feedback lose practice in empathy. When systems score performance and generate generic comments, managers have fewer reasons to listen closely to employees. Stuart Russell and Peter Norvig argue that intelligent reasoning relies on context, something machines still struggle to grasp (Russell and Norvig). When people outsource too much judgment, they weaken the abilities that make human decisions reliable.</p>
            <p>Information quality deepens the challenge. Large models can produce fluent writing that contains hidden errors or recycled bias. The text looks confident even when the claims are false. Many readers assume accuracy simply because the phrasing sounds smooth. Emily Bender and her colleagues warn that this illusion of understanding misleads readers and undermines public reasoning (Bender et al.). When people cannot distinguish evidence from confident imitation, civic understanding suffers. Classrooms, newsrooms, and courtrooms depend on careful distinctions between fact and speculation. AI generated writing blurs these lines when used without scrutiny.</p>
            <p>These issues point to a broader truth about responsibility. James Vincent of The Verge argues that technology amplifies the priorities of its users rather than the priorities of the code (Vincent). Meetings become shallow when leaders value speed more than care. Class discussions lose depth when students choose convenience over curiosity. The same tool can either thin out dialogue or deepen it depending on how it is framed, introduced, and discussed. Human choices remain central at every stage.</p>
            <p>To support better outcomes, governance must play a role. Clear standards should require transparency, limit unnecessary data collection, and mandate human oversight for decisions that affect rights or opportunities. These rules protect privacy and fairness while also supporting innovation. When people understand how systems work and what data they use, adoption becomes more stable. The OECD’s principles on trustworthy AI highlight the balance between innovation and accountability (OECD). Policies that require impact assessments and appeal processes help ensure that AI supplements human judgment rather than replaces it in secret.</p>
            <p>Yet policy alone is not enough. Daily practice shapes culture. Teams can preserve connection by reserving time for live problem solving and debate rather than relying solely on AI summaries. Students can draft essays without AI, then revise with assistance and include a note explaining how the tool was used. These habits train creativity, accountability, and empathy. They also reinforce the idea that technology supports thinking instead of replacing it.</p>
            <p>Similarly, clear norms can strengthen both classrooms and workplaces. Students might use AI for brainstorming or grammar checks while still writing the core analysis themselves. A brief explanation of how AI contributed increases transparency and gives teachers insight into where students struggle. In workplaces, treating AI as a junior collaborator that is helpful but not authoritative keeps judgment sharp and relationships strong. Colleagues learn how each person reasons, not just how each person writes a prompt.</p>
            <p>Culture ultimately determines how these tools shape connection. Institutions that reward speed and output often weaken relationships. Institutions that reward reflection protect them. The same technology can support either outcome depending on priorities. Equity adds another layer. When only well resourced organizations can afford strong guardrails, risks shift toward communities with fewer protections. Shared standards and public investment can help smaller schools and agencies implement AI safely and transparently.</p>
            <p>Despite these challenges, AI offers real benefits. It expands access to expertise, improves consistency in services, and creates new ways to learn and create. For many people, AI tools provide the first writing coach, language partner, or coding tutor available at any time. The challenge is to gain these advantages without losing the human contact that gives work and learning their meaning. Short policies, clear labels, and steady habits help keep AI grounded in accountability. Slowing down when needed protects trust. A simple statement that a person reviewed a decision can signal that responsibility still rests with humans.</p>
            <p>Ultimately, the central question is not whether AI helps or harms. It is how people choose to use it. Thoughtful adoption can strengthen knowledge and support fairness. Careless use can dull empathy and narrow curiosity. The future of human connection depends on our willingness to remain accountable, question convenience, and protect the attention we give one another. If institutions model that care, and if individuals practice it, AI reliance can grow without erasing the human presence that makes connection real.</p>
          </article>
        </div>
      </section>

      <!-- Draft 1 tab -->
      <section id="draft1" role="tabpanel" aria-labelledby="tab-draft1">
        <div class="card">
          <article>
            <h2>How reliance on AI shapes human connection (Draft 1)</h2>
            <div class="meta">By Brennan Shea. November 3, 2025.</div>
            <p>I use artificial intelligence every day. It helps me draft emails, plan tasks, and find patterns in data. These tools feel natural now. Yet as their use expands, I often wonder how this growing reliance changes the way people relate to one another. Artificial intelligence can lift productivity and widen access, but unchecked dependence may weaken real human connection. The goal is not to reject AI but to manage its place responsibly in modern life.</p>
            <p>To understand the appeal of AI, it helps to look at its practical benefits. Many users rely on AI to complete repetitive tasks quickly and accurately. It drafts notes, summarizes information, and corrects grammar errors, saving people both time and frustration. Lawson highlights these effects, noting how AI enhances communication and creativity for everyday users (Lawson). From my experience, using AI to organize schedules and generate drafts frees mental space for deeper thought and more meaningful conversation. This efficiency represents one of the technology’s greatest strengths.</p>
            <p>These personal advantages also extend to public good. In Southeast Asia, machine learning has improved weather prediction, forest monitoring, and waste management. Board documents these examples, showing that while infrastructure limits remain, the results prove promising (Board). Projects like these demonstrate how AI can serve as a global tool for safety, sustainability, and innovation. When systems prevent harm or protect natural resources, they validate their role as an ally to human progress.</p>
            <p>However, these benefits are paired with serious ethical concerns. The European Parliamentary Research Service warns that unregulated AI use increases surveillance, bias, and privacy loss. As organizations automate decision-making, small biases can scale into widespread inequality. People who feel monitored may avoid open dialogue or creativity (European Parliamentary Research Service). In this way, technology that should empower users can instead suppress trust. Without oversight, the systems meant to connect us could divide us further.</p>
            <p>Beyond issues of privacy, dependence on AI can erode critical thinking. Students who turn to chatbots for quick answers lose practice in analysis. Professionals who use digital assistants to write feedback risk losing empathy. Russell and Norvig explain that intelligent reasoning depends heavily on context, something machines still struggle to grasp (Russell and Norvig). When people outsource judgment to AI, they slowly lose the very skills that make human decision-making valuable.</p>
            <p>Some experts argue this erosion extends to information itself. Bender and colleagues describe the “stochastic parrots” problem, where language models generate fluent but unreliable text (Bender et al.). While these systems mimic understanding, they often reproduce existing biases or errors. The result is polished writing without accuracy. When audiences cannot tell truth from imitation, civic understanding weakens. This confusion threatens democracy as much as individual learning.</p>
            <p>Social scientists also warn that AI can widen emotional distance. Turkle observes that people increasingly substitute digital conversation for genuine connection (Turkle). AI could accelerate this by managing emotional exchanges on our behalf. For example, users can ask a chatbot to write apologies, compliments, or condolences. The words may be polite, but they lack the personal vulnerability that builds trust. Over time, emotional outsourcing could dull empathy and make real relationships harder to maintain.</p>
            <p>Still, some believe that technology itself is not to blame. Vincent argues that human priorities shape AI outcomes, not the algorithms alone (Vincent). A meeting becomes shallow only when leaders value speed over care. A class discussion loses meaning only when students choose convenience over curiosity. This argument reminds us that design and use are inseparable. Developers and users share equal responsibility for ensuring that tools serve human well-being.</p>
            <p>Protecting privacy is equally crucial. The OECD emphasizes transparency and human oversight as essential safeguards (OECD). Governments and corporations must restrict unnecessary data collection and enforce fairness audits for algorithms. Responsible regulation does not limit progress; it ensures that innovation remains ethical and sustainable. These standards keep AI grounded in respect for human dignity.</p>
            <p>Just as systems need oversight, people need practice. Teams should reserve time for live problem-solving, debate, and mentorship. Students should write first drafts without AI before using it for revision. These steps preserve creativity and empathy. In both workplaces and classrooms, collaboration grows stronger when people rely on dialogue, not automation, to solve problems.</p>
            <p>Education shows how balance can succeed. Teachers can integrate AI as a brainstorming aid while reinforcing original thought. Asking students to explain how AI shaped their work builds accountability and reflection. These habits teach digital ethics and reinforce the value of authenticity in writing and discussion. When guided carefully, AI becomes a tool for growth, not avoidance.</p>
            <p>Public institutions face similar challenges. They can apply AI to improve public safety, manage infrastructure, and expand access to information. Yet human oversight must remain central. Publishing clear reports about how AI systems operate and inviting citizen feedback promote transparency. Treating the public as collaborators rather than subjects builds civic trust and ensures that innovation aligns with shared values.</p>
            <p>In the end, the question is not whether AI helps or harms but how humans choose to use it. Artificial intelligence magnifies both our strengths and our weaknesses. It can expand knowledge, streamline systems, and improve lives. It can also reduce empathy, spread bias, and dull creativity if left unchecked. The future depends on our willingness to slow down, reflect, and stay accountable. By pairing efficiency with ethics, and curiosity with care, we can ensure that AI supports, not replaces, the human connection that defines us.</p>
          </article>
        </div>
      </section>

      <section id="works" role="tabpanel" aria-labelledby="tab-works">
        <div class="card">
          <h2>Works Cited</h2>
          <ol class="works">
            <li>Bender, Emily M., et al. "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big." <em>Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency</em>, 2021.</li>
            <li>Board, Jack. "AI in Southeast Asia: Machine Learning Offers New Solutions to Age-old Environmental Problems." Channel News Asia, 2024.</li>
            <li>European Parliamentary Research Service. <em>The Ethics of Artificial Intelligence: Issues and Initiatives</em>. European Parliament, 2020.</li>
            <li>Lawson, BriA’nna. "Enhancing Everyday Life: How AI is Revolutionizing Your Daily Experience." CEAMLS News, Morgan State University, 21 Nov. 2023.</li>
            <li>OECD. <em>OECD Principles on Artificial Intelligence</em>. Organisation for Economic Co-operation and Development, 2019.</li>
            <li>Russell, Stuart, and Peter Norvig. <em>Artificial Intelligence: A Modern Approach</em>. 4th ed., Pearson, 2021.</li>
            <li>Turkle, Sherry. <em>Alone Together: Why We Expect More from Technology and Less from Each Other</em>. Basic Books, 2011.</li>
            <li>Vincent, James. "The Problem with AI Isn’t Technology, it’s Humans." <em>The Verge</em>, 2023.</li>
          </ol>
        </div>
      </section>

      <section id="reflection" role="tabpanel" aria-labelledby="tab-reflection">
        <div class="card">
          <h2>Reflection</h2>
          <p>Writing this project taught me how to connect evidence, purpose, and audience into one sustained argument. At first, I saw research writing as formal and distant, but ENC 1102 helped me see it as a process of choice and clarity. I had to decide what claim mattered, why readers should care, and how sources could guide reasoning without taking control of it. My goal was to balance analysis and simplicity so that the essay could read like a conversation about real consequences. Revision played the biggest role in growth. Early drafts felt mechanical. Later ones focused more on cause and effect, tone, and smooth transitions. I also learned to cut vague language and choose verbs that show intent. Citing sources correctly built credibility and rhythm. More than anything, I learned to treat writing as an act of thinking out loud, where each paragraph tests an idea against evidence. This reflection represents how ENC 1102 changed my understanding of writing from a grade-driven task to a form of inquiry and expression. It is about learning to write with both discipline and curiosity, which are the same skills that make responsible research possible. Regarding AI, I have used models such as GPT-5 to overall ask for opinion on my work (how it can be objectively improved). In this sense, it helped me add context to my writing, given my audience's prior knowledge on AI before reading the blog.</p>
        </div>
      </section>
    </main>

    <footer>
      <div>© 2025 Brennan Shea.</div>
    </footer>
  </div>

  <script>
    const tabs = document.querySelectorAll('[role="tab"]');
    const panels = document.querySelectorAll('[role="tabpanel"]');
    function activate(id) {
      panels.forEach(p => {
        const isTarget = p.id === id;
        p.classList.toggle('active', isTarget);
        p.setAttribute('aria-hidden', String(!isTarget));
      });
      tabs.forEach(t => t.setAttribute('aria-selected', String(t.getAttribute('aria-controls') === id)));
      history.replaceState(null, '', '#' + id);
    }
    tabs.forEach(t => t.addEventListener('click', () => activate(t.getAttribute('aria-controls'))));
    const hash = location.hash.replace('#','');
    if (hash && document.getElementById(hash)) activate(hash);
  </script>
</body>
</html>
